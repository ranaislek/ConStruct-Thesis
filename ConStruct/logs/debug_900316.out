torch version: 2.0.1+cu118
CUDA available: True
CUDA device: NVIDIA RTX 2000 Ada Generation
<frozen importlib._bootstrap>:228: RuntimeWarning: to-Python converter for std::pair<double, double> already registered; second conversion method ignored.
[rank: 0] Global seed set to 0
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 0
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
Loading reference metrics.
Absorbing transition model with edges
[2025-07-03 13:59:04,452][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2025-07-03 13:59:04,452][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA RTX 2000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/rislek/.conda/envs/construct/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /extra/rislek/ConStruct-Thesis/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                  | Type                  | Params
----------------------------------------------------------------
0 | train_loss            | TrainLoss             | 0     
1 | train_metrics         | TrainMolecularMetrics | 0     
2 | val_metrics           | MetricCollection      | 0     
3 | val_nll               | NLL                   | 0     
4 | val_sampling_metrics  | SamplingMetrics       | 0     
5 | test_metrics          | MetricCollection      | 0     
6 | test_nll              | NLL                   | 0     
7 | test_sampling_metrics | SamplingMetrics       | 0     
8 | model                 | GraphTransformer      | 1.5 K 
----------------------------------------------------------------
1.5 K     Trainable params
0         Non-trainable params
1.5 K     Total params
0.006     Total estimated model params size (MB)
wandb: Tracking run with wandb version 0.15.4
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 0: val/epoch_NLL: 56.54 -- val/X_kl: 3.35 -- val/E_kl: 43.92 -- val/charges_kl: 1.01 
Val loss: 56.5358 	 Best val loss:  56.5358

Val epoch 0 ends
Starting epoch 0
[2025-07-03 13:59:09,230][torch.nn.parallel.distributed][INFO] - Reducer buckets have been rebuilt in this iteration.
`Trainer.fit` stopped: `max_epochs=1` reached.
[rank: 0] Global seed set to 0
You are using a CUDA device ('NVIDIA RTX 2000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                       epoch ▁
wandb:            train/AromaticCE █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              train/CarbonCE █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              train/DoubleCE █▅▃▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:               train/FluorCE █▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/NitroCE █▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              train/NoBondCE █▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/OxyCE ▇█▇▇▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb:              train/SingleCE ▇██▇▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:              train/TripleCE █▇▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_epoch/E_CE ▁
wandb:      train_epoch/charges_CE ▁
wandb: train_epoch/epochAromaticCE ▁
wandb:   train_epoch/epochCarbonCE ▁
wandb:   train_epoch/epochDoubleCE ▁
wandb:    train_epoch/epochFluorCE ▁
wandb:    train_epoch/epochNitroCE ▁
wandb:   train_epoch/epochNoBondCE ▁
wandb:      train_epoch/epochOxyCE ▁
wandb:   train_epoch/epochSingleCE ▁
wandb:   train_epoch/epochTripleCE ▁
wandb:            train_epoch/x_CE ▁
wandb:            train_epoch/y_CE ▁
wandb:             train_loss/E_CE █▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_loss/X_CE █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss/batch_loss █▆▆▃▃▄▂▄▂▂▃▂▁▃▁▃▄▁▂▃▂▂▁▃▂▃▂▂▃▂▁▃▂▃▂▂▂▂▃▃
wandb:       train_loss/charges_CE █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_loss/y_CE ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                    val/E_kl ▁█
wandb:                    val/X_kl ▁█
wandb:              val/charges_kl █▁
wandb:               val/epoch_NLL ▁█
wandb: 
wandb: Run summary:
wandb:                       epoch 0
wandb:            train/AromaticCE 0.00504
wandb:              train/CarbonCE 0.57798
wandb:              train/DoubleCE 0.11143
wandb:               train/FluorCE 0.02185
wandb:               train/NitroCE 0.35044
wandb:              train/NoBondCE 0.51799
wandb:                 train/OxyCE 0.42638
wandb:              train/SingleCE 0.48675
wandb:              train/TripleCE 0.0523
wandb:            train_epoch/E_CE 0.64296
wandb:      train_epoch/charges_CE 0.14666
wandb: train_epoch/epochAromaticCE 0.00503
wandb:   train_epoch/epochCarbonCE 0.57797
wandb:   train_epoch/epochDoubleCE 0.11142
wandb:    train_epoch/epochFluorCE 0.02188
wandb:    train_epoch/epochNitroCE 0.35041
wandb:   train_epoch/epochNoBondCE 0.51799
wandb:      train_epoch/epochOxyCE 0.42637
wandb:   train_epoch/epochSingleCE 0.48675
wandb:   train_epoch/epochTripleCE 0.05229
wandb:            train_epoch/x_CE 0.77677
wandb:            train_epoch/y_CE -1.0
wandb:             train_loss/E_CE 3.21493
wandb:             train_loss/X_CE 0.77677
wandb:       train_loss/batch_loss 3.48078
wandb:       train_loss/charges_CE 0.29331
wandb:             train_loss/y_CE -1.0
wandb:                    val/E_kl 107.04414
wandb:                    val/X_kl 3.3841
wandb:              val/charges_kl 0.57798
wandb:               val/epoch_NLL 111.56018
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /extra/rislek/ConStruct-Thesis/wandb/offline-run-20250703_135905-frmb5hbp
wandb: Find logs at: ./wandb/offline-run-20250703_135905-frmb5hbp/logs
wandb: Tracking run with wandb version 0.15.4
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Epoch 0: val/epoch_NLL: 111.56 -- val/X_kl: 3.38 -- val/E_kl: 107.04 -- val/charges_kl: 0.58 
Val loss: 111.5602 	 Best val loss:  56.5358

Val epoch 0 ends
Train epoch 0 ends
Epoch 0 finished: X: 0.78 -- charges: 0.15 -- E: 0.64 -- y: -1.00 -- 294.3s 
Epoch 0: {'CarbonCE': 0.578, 'NitroCE': 0.35, 'OxyCE': 0.426, 'FluorCE': 0.022} -- {'NoBondCE': 0.518, 'SingleCE': 0.487, 'DoubleCE': 0.111, 'TripleCE': 0.052, 'AromaticCE': 0.005}
Test loss: 111.4470
Epoch 1: test/epoch_NLL: 111.4470 -- test/X_kl: 3.4373 -- test/E_kl: 106.8591 -- test/charges_kl: 0.5952 
Sampling start on GR0
Samples to generate: 1 for each of the 1 devices
Samples to save: 1
Sampling a batch with 1 graphs on local rank 0. Saving 1 visualization and 1 full chains.
Batch sampled. Visualizing chains starts!
1/1 chains saved on local rank 0.Saving /extra/rislek/ConStruct-Thesis/chains/epoch1/batch1_GR0/molecule_1_0.gif to wandb
Visualizing 1 graphs out of 1
Saving /extra/rislek/ConStruct-Thesis/graphs/qm9/epoch1_b1/graph_0.png to wandb
Done.
Saving the generated graphs
Saved.
Computing sampling metrics...
Error messages: AtomValence 0, Kekulize 0, other 0,  -- No error 1
Some generated smiles: COCC(C)C(O)O
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                        test/E_kl ▁
wandb:                        test/X_kl ▁
wandb:                  test/charges_kl ▁
wandb:                   test/epoch_NLL ▁
wandb:               test_ratio/average ▁
wandb:             test_ratio/fcd score ▁
wandb:           test_sampling/ChargeW1 ▁
wandb:       test_sampling/Disconnected ▁
wandb:        test_sampling/EdgeTypesTV ▁
wandb:      test_sampling/MaxComponents ▁
wandb:     test_sampling/MeanComponents ▁
wandb:        test_sampling/NodeTypesTV ▁
wandb:            test_sampling/Novelty ▁
wandb:         test_sampling/NumNodesW1 ▁
wandb:         test_sampling/Uniqueness ▁
wandb:          test_sampling/ValencyW1 ▁
wandb:           test_sampling/Validity ▁
wandb:          test_sampling/fcd score ▁
wandb: test_sampling/lobster_components ▁
wandb:          test_sampling/no_cycles ▁
wandb:          test_sampling/planarity ▁
wandb: 
wandb: Run summary:
wandb:                        test/E_kl 106.85912
wandb:                        test/X_kl 3.43726
wandb:                  test/charges_kl 0.59519
wandb:                   test/epoch_NLL 111.44704
wandb:               test_ratio/average -1.15889
wandb:             test_ratio/fcd score -1.15889
wandb:           test_sampling/ChargeW1 0.2294
wandb:       test_sampling/Disconnected 100.0
wandb:        test_sampling/EdgeTypesTV 0.20241
wandb:      test_sampling/MaxComponents 2.0
wandb:     test_sampling/MeanComponents 2.0
wandb:        test_sampling/NodeTypesTV 0.34887
wandb:            test_sampling/Novelty 100.0
wandb:         test_sampling/NumNodesW1 0.20837
wandb:         test_sampling/Uniqueness 100.0
wandb:          test_sampling/ValencyW1 1.21637
wandb:           test_sampling/Validity 100.0
wandb:          test_sampling/fcd score -1
wandb: test_sampling/lobster_components 1.0
wandb:          test_sampling/no_cycles 1.0
wandb:          test_sampling/planarity 1.0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /extra/rislek/ConStruct-Thesis/wandb/offline-run-20250703_140403-z0vaeese
wandb: Find logs at: ./wandb/offline-run-20250703_140403-z0vaeese/logs
Not enough (<=1) valid smiles for FCD computation.
FCD score: -1
All smiles saved on rank 0
Molecular metrics computed.
Sampling metrics {'test_sampling/NumNodesW1': 0.208, 'test_sampling/NodeTypesTV': 0.349, 'test_sampling/EdgeTypesTV': 0.202, 'test_sampling/Disconnected': 100.0, 'test_sampling/MeanComponents': 2.0, 'test_sampling/MaxComponents': 2.0, 'test_sampling/planarity': 1.0, 'test_sampling/no_cycles': 1.0, 'test_sampling/lobster_components': 1.0, 'test_sampling/generated_deg_hist': [0.111, 0.444, 0.222, 0.222, 0.0, 0.0, 0.0, 0.0, 0.0], 'test_sampling/diff_deg_hist': [0.111, 0.205, -0.211, -0.051, -0.053, -0.001, 0.0, 0.0, 0.0], 'test_sampling/abs_diff_deg_hist': [0.111, 0.205, 0.211, 0.051, 0.053, 0.001, 0.0, 0.0, 0.0], 'test_sampling/Validity': 100.0, 'test_sampling/Uniqueness': 100.0, 'test_sampling/Novelty': 100.0, 'test_sampling/fcd score': -1, 'test_sampling/ChargeW1': 0.229, 'test_sampling/ValencyW1': 1.216, 'test_ratio/fcd score': -1.159, 'test_ratio/average': -1.159}
Done. Sampling took 0.98 seconds

Test ends.
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
        test/E_kl           106.85912322998047
        test/X_kl            3.437258005142212
     test/charges_kl        0.5951911807060242
     test/epoch_NLL          111.4470443725586
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
